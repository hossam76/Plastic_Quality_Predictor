# -*- coding: utf-8 -*-
"""COURSEWORK.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_SlTFlMi8skmPZIRtE3LCz1CfIDhdbFv
"""

#installing libraries
!pip install matplotlib
!pip install catboost==1.2.0 numpy==1.23.5

#importing libraries

import pandas as pd
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats


from scipy.stats import skew, kurtosis
from scipy.stats import zscore
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from catboost import CatBoostClassifier
from sklearn.neural_network import MLPClassifier
from scipy.stats import zscore

!pip show catboost

!pip install catboost==1.2.0 numpy==1.23.5

"""**data overview**"""

#loading the dataset and viewing first five rows

df=pd.read_csv('/content/drive/MyDrive/Cs/CW_Dataset_4134124.csv')
df.head(5)

# checking columns,data types, missing values

print(df.info())

#viewing first five rows
print(df.head(5))

#checking the shape of the dataset.

print(f"Dataset contains {df.shape[0]} rows and {df.shape[1]} columns.")
print("Number of instances:", df.shape[0])
print("Number of features:", df.shape[1] - 1)  # minus target

#checking the data types

print(df.dtypes)

# Target class distribution
print("\nTarget class distribution:")
print(df['quality'].value_counts())


#plotting
plt.figure(figsize=(6,3))
sns.countplot(data=df, x='quality')
plt.xticks(ticks=[0, 1, 2, 3], labels=['Waste (1)', 'Acceptable (2)', 'Target (3)', 'Inefficient (4)'])
plt.title('Target Class Distribution')
plt.show()

#chcking the summary statistics of numerical column
df.describe()

#numerical columns
numerical_columns = df.select_dtypes(include=['number']).columns
numerical_columns

#detecting imbalanced data(measuring skewness, kurtosis)
desc_stats_skewness = df.drop(columns='quality').apply(skew)
desc_stats_kurtosis= df.drop(columns='quality').apply(kurtosis)

print(f'skenwness: \n{desc_stats_skewness}')
print(f'kurtosis: \n{desc_stats_kurtosis}')





#Skewness > 0 ‚Üí Right-skewed (long tail on right)
#Skewness < 0 ‚Üí Left-skewed (long tail on left)

"""**Data cleaning**"""

# Count missing values per column
print(df.isnull().sum())

#checking unique values
print(f"Unique values:\n{df.nunique()}")

#checking the duplicate rows in the dataset
print(f"Duplicate rows: {df.duplicated().sum()}")

#counting the outliers
col_outlier=[]


def count_outliers_iqr(data):

    outlier_counts = {}
    for col in data.select_dtypes(include=['number']).columns:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        # Count outliers
        outliers = ((df[col] < lower_bound)
                    | (df[col] > upper_bound)).sum()
        outlier_counts[col] = outliers

        if outliers>0:
          col_outlier.append(col)
    return outlier_counts



# Get outlier counts
outlier_counts = count_outliers_iqr(df)

print("Outlier count per column (IQR Method)\n")
for columns, count in outlier_counts.items():
    print(f"{columns}: {count}")

#visualizing the outliers in the dataset
plt.figsize=(10,5)
sns.boxplot(df.select_dtypes(include=['number']))
plt.xticks(rotation=90)
plt.show()

# Creating a copy of the dataset to avoid modifying the original
data= df.copy()

# Removing or replacing outliers
for col in col_outlier:

    # Calculate IQR and bounds
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1

    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # If outliers number is low (<= 30), remove them
    if outlier_counts[col] <= 30:
        data= data[(data[col] >= lower_bound) & (data[col] <= upper_bound)]
        print(f"Outliers removed for {col}. Number of rows after removal: {data.shape[0]}")

    # If outliers are too many, replace them
    #based on the skewness

    else:
        skewness = abs(data[col].skew())  # Get skewness of the column

        if skewness >0.55:  # Skewed data ‚Üíreplacing by Median
            median_value = data[col].median()
            data[col] = data[col].apply(lambda x: median_value
                                                        if x < lower_bound or
                                                        x > upper_bound
                                                        else x)

        else:  # Normal distribution ‚Üí replacing by Mean
            mean_value = data[col].mean()
            data[col] = data[col].apply(lambda x: mean_value
                                                        if x < lower_bound or
                                                        x > upper_bound
                                                        else x)

data.shape #shape after outliers removal

"""**feature engineering**"""

#new features have been added that could potentially increase predictive power

data['Pressure_Efficiency'] = data['APVs - Specific injection pressure peak value'] / data['SKx - Closing force']
data['Torque_Efficiency'] = data['Ms - Torque peak value current cycle'] / data['Mm - Torque mean value current cycle']
data['Filling_Speed'] = data['SVo - Shot volume'] / data['time_to_fill']
data['Thermal_Delta'] = data['Melt temperature'] - data['Mold temperature']

#no constant valued features
#no low variance containing features
data.shape

#analyzing corelation
correlation_matrix = data.corr()
correlation_matrix

#viasualising the correlation
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt

#extracting upper triangle of the corelation
upper_tri=correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape),k=1).astype(bool))

#detecting highly corelated features
high_corr_features=[col for col in upper_tri.columns if any(upper_tri[col]>0.9)]

print(f"Number of highly correlated features: {len(high_corr_features)}")

#based on the domain knowledge it seems likely all the features are important.

"""#**data visualization**"""

# Select numeric columns
numeric_cols = data.select_dtypes(include='number').columns
n_cols = len(numeric_cols)

# Define subplot grid dimensions
n_cols_subplot = 4  # Number of plots per row
n_rows_subplot = int(np.ceil(n_cols / n_cols_subplot))  # Calculate required number of rows

# Create subplots grid
fig, axes = plt.subplots(n_rows_subplot, n_cols_subplot, figsize=(20, n_rows_subplot * 4))
axes = axes.flatten()  # Flatten axes array for easy iteration

# Plot histograms with KDE for each numeric feature
for i, col in enumerate(numeric_cols):
    sns.histplot(data[col], kde=True, bins=30, ax=axes[i], color='blue', edgecolor='black')
    axes[i].set_title(f'Distribution of {col}', fontsize=12)
    axes[i].set_xlabel('')
    axes[i].set_ylabel('Frequency')

# Turn off unused subplots (if any exist)
for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

# Adjust layout
plt.tight_layout()
plt.suptitle("Histograms with KDE of Numerical Features", fontsize=18, y=1.02)
plt.show()

numeric_cols = data.select_dtypes(include='number').columns

# Plot each numerical feature with KDE overlay
for col in numeric_cols:
    plt.figure(figsize=(8, 4))
    sns.histplot(data=data, x=col, kde=True, bins=30, color='skyblue', edgecolor='black')
    plt.title(f"Distribution of '{col}' with KDE Line")
    plt.xlabel(col)
    plt.ylabel("Frequency")
    plt.grid(True)
    plt.tight_layout()
    plt.show()

# Select a few relevant features (too many will make the plot unreadable)
selected_features = [
    'Melt temperature',
    'time_to_fill',
    'SVo - Shot volume',
    'SKs - Clamping force peak value',
    'APVs - Specific injection pressure peak value',
    'quality'  # must include target
]

# Create a pairplot with hue set to quality class
sns.pairplot(data[selected_features], hue='quality', diag_kind='kde', palette='Set2')
plt.suptitle("Pairplot of Selected Features by Quality Class", y=1.02)
plt.show()

"""**Data preparation and analysis**"""

#no missing values
#no duplicate rows
#not necessary to change the data types
#handled outliers
#no need to encode any variable as all of them numerical target variable
#need to normalize numerical feathures
#splitting the dataset
#applying cross validation folds

#separating features and target variables
X=data.drop(columns=['quality'])
y=data['quality']

#anova test for feature selection
anova_results={}

for feature in X.columns:
  f_stat,p_value=stats.f_oneway(
      *[X[feature][y==cls]for cls in y.unique()]
  )
  anova_results[feature]=(f_stat,p_value)

# Convert results to DataFrame
anova_df = pd.DataFrame.from_dict(anova_results, orient='index', columns=['F-Statistic', 'p-Value'])

# Sort by significance
anova_df = anova_df.sort_values(by='p-Value')
anova_df

# Select only significant features (p-value < 0.05)
significant_features = anova_df[anova_df["p-Value"] < 0.05].index.tolist()

print(f"Selected {len(significant_features)} important features out of {data.shape[1] - 1}.")

# Sorting the ANOVA results by F-statistic for better visualization
anova_dff = anova_df.sort_values(by="F-Statistic", ascending=False).dropna()

# Plot the F-statistic values
plt.figure(figsize=(12, 6))
plt.barh(anova_dff.index, anova_dff["F-Statistic"], color="blue")
plt.xlabel("F-statistic Value")
plt.ylabel("Features")
plt.title("ANOVA F-statistic for Each Feature")
plt.gca().invert_yaxis()  # Invert y-axis for better readability
plt.show()

# Create a new dataset with only significant features
X_selected = data[significant_features]
y= data["quality"]  # Keeping target variable unchanged

sns.boxplot(x='quality', y='ZUx - Cycle time', data=data)
plt.title('Cycle Time vs Quality Class')

from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.feature_selection import VarianceThreshold


var_thresh = VarianceThreshold(threshold=0.01)
X_low_var_filtered = var_thresh.fit_transform(data)
kept_columns = data.columns[var_thresh.get_support()]


vif_data = pd.DataFrame()
vif_data["Feature"] = kept_columns
vif_data["VIF"] = [variance_inflation_factor(X_low_var_filtered, i) for i in range(X_low_var_filtered.shape[1])]
vif_data = vif_data.sort_values(by="VIF", ascending=False)

vif_data

from scipy.stats import f_oneway
import pandas as pd
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.feature_selection import VarianceThreshold

# Step 1: Remove low variance features
var_thresh = VarianceThreshold(threshold=0.01)
X_low_var_filtered = var_thresh.fit_transform(data.drop(columns=['quality']))
kept_columns = data.drop(columns=['quality']).columns[var_thresh.get_support()]
X_filtered = pd.DataFrame(X_low_var_filtered, columns=kept_columns)

# Step 2: Calculate VIF
vif_data = pd.DataFrame()
vif_data["Feature"] = kept_columns
vif_data["VIF"] = [variance_inflation_factor(X_filtered.values, i) for i in range(X_filtered.shape[1])]

# Step 3: Calculate ANOVA P-values
anova_pvals = []
for feature in kept_columns:
    try:
        groups = [group[feature].values for name, group in data.groupby('quality')]
        _, pval = f_oneway(*groups)
        anova_pvals.append(pval)
    except:
        anova_pvals.append(None)

vif_data["ANOVA_P_Value"] = anova_pvals

# Step 4: Sort or filter if needed
vif_data_sorted = vif_data.sort_values(by="ANOVA_P_Value")
vif_data_sorted

selected_features = [
    'ZUx - Cycle time',                # Strongest overall indicator
    'ZDx - Plasticizing time',         # High correlation, High VIF
    'APVs - Specific injection pressure peak value', # Moderate correlation, low VIF
    'time_to_fill', #high predictive power
    'Filling_Speed', # Engineered, time-volume balance
    'SVo - Shot volume',               # Moderate correlation, low VIF
    'Mm - Torque mean value current cycle',# Engineered, may add useful signal

]

"""# **normalization and splitting the dataset**"""

# Prepare final datasets
X_final = data[selected_features]
y_final = data['quality']

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X_final, y_final, test_size=0.3, random_state=42, stratify=y_final
)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Output shapes
final_shapes = {
    "Selected Features": selected_features,
    "X_train shape": X_train_scaled.shape,
    "X_test shape": X_test_scaled.shape,
    "y_train shape": y_train.shape,
    "y_test shape": y_test.shape
}

final_shapes

#normalisation is basically a method to ensure
#that all the fetures have same scale and to prevent ceratin features
#to prevent dominating the learning process

"""**model development**"""



# üîß Step 1: Install/Upgrade Required Libraries

!pip install --upgrade numpy --quiet  # Upgrade numpy first
!pip install --upgrade catboost scikit-learn pandas matplotlib seaborn --quiet

# üîç Step 2: Import Libraries
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from catboost import CatBoostClassifier
from sklearn.neural_network import MLPClassifier
from scipy.stats import zscore

#Defining  Models with default parameters.
models = {
    'Random Forest': RandomForestClassifier(random_state=42),
    'KNN': KNeighborsClassifier(),
    'SVM': SVC(probability=True, random_state=42),
    'CatBoost': CatBoostClassifier(verbose=0, random_state=42),
    'ANN': MLPClassifier(random_state=42, max_iter=500)
}

#Training and Evaluating each models.
results = {}
for name, model in models.items():
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)
    report = classification_report(y_test, y_pred, output_dict=True)
    results[name] =pd.DataFrame(report).transpose()

# displaying the result
for name,result in results.items():
  print(f'\n classification report for {name}:\n')
  print(result)

#cross validation

from sklearn.model_selection import cross_val_score

cv_scores = {}

for name, model in models.items():
    score = cross_val_score(model, X_train_scaled, y_train, cv=10, scoring='accuracy').mean()
    cv_scores[name] = round(score, 4)

# Create CV score table
cv_df = pd.DataFrame.from_dict(cv_scores, orient='index', columns=['10-Fold CV Accuracy'])
cv_df = cv_df.sort_values(by='10-Fold CV Accuracy', ascending=False)
cv_df

#Bar plot of model accuracy
cv_df.plot(kind='barh', legend=False, color='skyblue')
plt.title("Model Comparison - Cross-Validation Accuracy")
plt.xlabel("Accuracy")
plt.xlim(0, 1)
plt.grid(axis='x')
plt.show()

#hyperparameter tuning with GridSearchCv
#Defining  hyperparameter grids for each model
tuning_configs = {
    "Random Forest": {
        'model': RandomForestClassifier(random_state=42),
        'params': {
            'n_estimators': [100, 200],
            'max_depth': [None, 10],
            'min_samples_split': [2, 5]
        }
    },
    "SVM": {
        'model': SVC(probability=True, random_state=42),
        'params': {
            'C': [1, 10],
            'kernel': ['rbf', 'linear']
        }
    },
    "KNN": {
        'model': KNeighborsClassifier(),
        'params': {
            'n_neighbors': [3, 5, 7],
            'weights': ['uniform', 'distance']
        }
    },
    "ANN": {
        'model': MLPClassifier(max_iter=500, random_state=42),
        'params': {
            'hidden_layer_sizes': [(50,), (100,)],
            'activation': ['relu', 'tanh']
        }
    },
    "CatBoost": {
        'model': CatBoostClassifier(verbose=0, random_state=42),
        'params': {
            'depth': [4, 6],
            'learning_rate': [0.03, 0.1],
            'iterations': [100, 200]
        }
    }
}

# Running GridSearchCV for all models and evaluate
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
best_models = {}
for name, config in tuning_configs.items():
    print(f"\nüîç Tuning {name}...")
    grid = GridSearchCV(estimator=config['model'], param_grid=config['params'], cv=10, scoring='accuracy', n_jobs=-1)
    grid.fit(X_train_scaled, y_train)
    best_model = grid.best_estimator_
    y_pred = best_model.predict(X_test_scaled)
    report = classification_report(y_test, y_pred, output_dict=True)
    cm = confusion_matrix(y_test, y_pred)

    best_models[name] = {
        'Best Params': grid.best_params_,
        'CV Score': grid.best_score_,
        'Report': pd.DataFrame(report).transpose(),
        'Confusion Matrix': cm,
        'Estimator': best_model
    }

    print(f"Best Parameters for {name}: {grid.best_params_}")
    print(f"Cross-Validation Accuracy: {grid.best_score_:.4f}")
    ConfusionMatrixDisplay(confusion_matrix=cm).plot(cmap='Blues')
    plt.title(f"{name} - Confusion Matrix")
    plt.show()

# creating Summary Table
summary_data = {
    model: [round(data['CV Score'], 4)] for model, data in best_models.items()
}
summary_df = pd.DataFrame.from_dict(summary_data, orient='index', columns=['Tuned 10-Fold CV Accuracy'])
display(summary_df.sort_values(by='Tuned 10-Fold CV Accuracy', ascending=False))

#as both randomforest and catboost got the same cv accuracy , we looked
#on the secondary selection methods(confusion matrix,f1 scores and preferred to
#select randomforest

# Save best model
import joblib
joblib.dump(best_models['Random Forest']['Estimator'], 'best_model.pkl')

joblib.dump(scaler, 'scaler.pkl')

"""#**Dashboard**"""

!pip install streamlit

selected_features

# app.py ‚Äî Streamlit Dashboard for Product Quality Prediction
!pip install pyngrok --quiet
app_code= '''
import streamlit as st
st.set_page_config(page_title="Plastic Quality Predictor", layout="wide")

#importing libraries
import pandas as pd
import joblib
import io
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay



# Load trained model and scaler
model = joblib.load('best_model.pkl')
scaler = joblib.load('scaler.pkl')

# Expected features used in model training
expected_features = [
    "ZUx - Cycle time",
    "ZDx - Plasticizing time",
    "APVs - Specific injection pressure peak value",
    "time_to_fill",
    "Filling_Speed",
    "SVo - Shot volume",
    "Mm - Torque mean value current cycle"
]

# Map predicted labels to class names
label_map = {
    1: "Waste",
    2: "Acceptable",
    3: "Target",
    4: "Inefficient"
}

# Custom CSS to style the st.sidebar.info() messages
custom_css = """
<style>
    /* Target the st.sidebar.info() alert boxes */
    [data-testid="stSidebar"] .stAlert {
        background-color: #e0f7fa; /* Light cyan background */
        color: #006064; /* Dark cyan text */
        border-left: 5px solid #004d40; /* Darker cyan left border */
    }
</style>
"""

# Inject the custom CSS into the Streamlit app
st.markdown(custom_css, unsafe_allow_html=True)



# Streamlit UI setup
st.title("üîç Plastic Injection Moulding ‚Äì Quality Predictor Dashboard")
st.markdown("Welcome! Use the tabs below to run predictions and review your model.")

st.sidebar.title("Plastic Injection Moulding- Quality Class Predictor")
st.sidebar.info("Navigate sections from the tabs above to use the model.")
st.sidebar.info("""
**Features:**
- Random Forest Classifier with 93.17% cross-validation accuracy
- Selected Parameters: Cycle time, Plasticizing time, Shot volume, Screw position, Torque Efficiency, Filling Speed
- Streamlit Interface for real-time single and batch predictions
- Model Performance Insights with visual analytics
- Exportable Results for decision-making and traceability
""")

tab1, tab2, tab3 = st.tabs(["üìà Single Prediction", "üìÅ Batch Prediction", "üìò Model Info"])

# ----- Single Prediction Tab -----
with tab1:
    st.subheader("üìà Single Quality Prediction")
    col1, col2 = st.columns(2)
    features = {}

    with col1:
        features["ZUx - Cycle time"] = st.number_input("Cycle Time (ZUx)", min_value=0.0, value=2.0)
        features["APVs - Specific injection pressure peak value"] = st.number_input("Injection Pressure Peak (APVs)", min_value=0.0, value=150.0)
        features["Filling_Speed"] = st.number_input("Filling Speed", min_value=0.0, value=3.5)
        features["Mm - Torque mean value current cycle"] = st.number_input("Torque Mean (Mm)", min_value=0.0, value=0.8)

    with col2:
        features["ZDx - Plasticizing time"] = st.number_input("Plasticizing Time (ZDx)", min_value=0.0, value=1.0)
        features["time_to_fill"] = st.number_input("Time to Fill", min_value=0.0, value=1.5)
        features["SVo - Shot volume"] = st.number_input("Shot Volume (SVo)", min_value=0.0, value=20.0)

    if st.button("üîç Predict Quality Class"):
        input_data = np.array([list(features.values())])
        input_scaled = scaler.transform(input_data)
        prediction = model.predict(input_scaled)[0]
        pred_label = label_map.get(int(prediction), "Unknown")
        st.success(f"üß† **Predicted Quality Class:** {int(prediction)} ‚Äî {pred_label}")

# ----- Batch Prediction Tab -----
with tab2:
    st.subheader("üìÅ Upload CSV for Batch Prediction")
    st.info("Ensure your CSV file includes the following columns:")
    st.code(", ".join(expected_features))

    uploaded_file = st.file_uploader("Upload CSV", type=["csv"])
    if uploaded_file is not None:
        data = pd.read_csv(uploaded_file)
        st.write("üìã Preview of Uploaded Data", data.head())

        try:
            data_for_prediction = data[expected_features]
            scaled_data = scaler.transform(data_for_prediction)
            predictions = model.predict(scaled_data)
            data['Predicted Quality'] = predictions
            data['Predicted Label'] = data['Predicted Quality'].map(label_map)

            st.success("‚úÖ Predictions completed successfully.")
            st.write("üîé Prediction Results:", data.head())

            st.subheader("üìä Class Distribution")
            class_counts = data['Predicted Label'].value_counts().sort_index()
            st.bar_chart(class_counts)

            if 'true_quality' in data.columns:
                st.subheader("üß© Confusion Matrix (True vs Predicted)")
                cm = confusion_matrix(data['true_quality'], data['Predicted Quality'])
                fig, ax = plt.subplots()
                disp = ConfusionMatrixDisplay(confusion_matrix=cm)
                disp.plot(ax=ax, cmap="Blues", colorbar=False)
                st.pyplot(fig)

            # Downloadable predictions
            csv_buffer = io.StringIO()
            data.to_csv(csv_buffer, index=False)
            st.download_button("üì• Download Results CSV", data=csv_buffer.getvalue(), file_name="predicted_output.csv", mime="text/csv")

        except KeyError as e:
            st.error(f"‚ùå Missing required columns: {e}")

# ----- Model Info Tab -----
with tab3:
    st.subheader("üìò Model Information")
    st.write(model)

    if hasattr(model, 'feature_importances_'):
        with st.expander("üìä Feature Importances"):
            importance_df = pd.DataFrame({
                'Feature': expected_features,
                'Importance': model.feature_importances_
            }).sort_values(by='Importance', ascending=True)

            fig, ax = plt.subplots()
            importance_df.plot(kind='barh', x='Feature', y='Importance', ax=ax, legend=False, color='#4CAF50')
            ax.set_title("Feature Importances", fontsize=14)
            ax.invert_yaxis()
            st.pyplot(fig)

    st.subheader("üìä Test Set Confusion Matrix")
    test_file = st.file_uploader("Upload test CSV with 'true_quality' column", type=["csv"], key="test_csv")
    if test_file is not None:
        test_df = pd.read_csv(test_file)
        try:
            X_test = test_df[expected_features]
            y_test = test_df["true_quality"]
            X_scaled = scaler.transform(X_test)
            y_pred = model.predict(X_scaled)

            cm = confusion_matrix(y_test, y_pred)
            fig, ax = plt.subplots()
            disp = ConfusionMatrixDisplay(confusion_matrix=cm)
            disp.plot(ax=ax, cmap="Oranges", colorbar=False)
            st.pyplot(fig)
        except KeyError as e:
            st.error(f"‚ùå Missing columns: {e}")

# Footer
st.markdown("---")
st.markdown("üìò *Created for ARI Coursework ‚Äî Streamlit Dashboard by 4134124*")

'''

with open('app.py', 'w') as f:
    f.write(app_code)

#downlaoding the necessary files
from google.colab import files
files.download('best_model.pkl')
files.download('scaler.pkl')
files.download('app.py')

"""#***launching dashboard***"""

#clearing caches and any other active pyngrok
!streamlit cache clear
from pyngrok import ngrok
ngrok.kill()

# for manual check :https://dashboard.ngrok.com/agents

#uploading the files
from google.colab import files
uploaded=files.upload()

from pyngrok import ngrok, conf
import os, time

# üîê Set authtoken securely
conf.get_default().auth_token = "2vHXgoK5uYjGPXsJIEnQWBH9kcx_4AqwuGMFnQgH94SWZ19vd"

# üßº Kill any existing tunnel
ngrok.kill()

# üöÄ Run Streamlit
os.system("streamlit run app.py &")
time.sleep(3)

# üåç Public URL
# Specify the port within the 'addr' parameter for HTTP tunnels
public_url = ngrok.connect(addr="8501")
print("‚úÖ Streamlit app is live at:", public_url)

"""**fetching the test dataset**"""

#fetching the test dataset
test_df = pd.DataFrame(X_test, columns=selected_features)
test_df["true_quality"] = y_test.values
test=test_df.to_csv("test_data_with_true_labels.csv", index=False)

print(test_df.head(5))

from google.colab import files
files.download('test_data_with_true_labels.csv')

